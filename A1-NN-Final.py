# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16hMwvDjFL1T4nMJyrmfK1dhHK-CS2NMp
"""

import pandas as pd

# Load Wine Quality Dataset
wine_data_path = "/content/Wine-quality/winequality-white.csv"
wine_data = pd.read_csv(wine_data_path, delimiter=';')

# # Display the first few rows of the dataset
# print(wine_data.head())

# # Check for missing values
# print(wine_data.isnull().sum())

# # Summary statistics
# print(wine_data.describe())

# # Handle missing values (if any)
# # For example, we can fill missing values with the mean of the column
# wine_data.fillna(wine_data.mean(), inplace=True)

# # Verify that there are no more missing values
# print(wine_data.isnull().sum())

# # Optionally, display the first few rows to verify the data
# print(wine_data.head())


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Separate features and target variable
X = wine_data.drop('quality', axis=1)
y = wine_data['quality']

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# # Verify the shapes of the training and testing sets
# print("X_train shape:", X_train.shape)
# print("X_test shape:", X_test.shape)
# print("y_train shape:", y_train.shape)
# print("y_test shape:", y_test.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

# Define the neural network model
def create_model(input_dim, hidden_layer_sizes=(100,), activation='relu'):
    model = Sequential()
    model.add(Dense(hidden_layer_sizes[0], input_dim=input_dim, activation=activation))
    for units in hidden_layer_sizes[1:]:
        model.add(Dense(units, activation=activation))
    model.add(Dense(1, activation='linear'))  # Use 'linear' activation for regression
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])  # 'mae' for mean absolute error
    return model

# Create the model
input_dim = X_train.shape[1]
model = create_model(input_dim)

# Train the model and capture the history
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)

# Plot the learning curves
plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Learning Curve')
plt.show()

# Evaluate the model on the test set
test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)
print(f'Test MAE: {test_mae}')


from sklearn.model_selection import learning_curve
from sklearn.model_selection import KFold

def plot_validation_curve(param_name, param_range, create_model_fn, X_train, y_train, X_test, y_test, epochs=100):
    train_scores = []
    test_scores = []

    for param in param_range:
        if param_name == 'hidden_layer_sizes':
            model = create_model_fn(input_dim=X_train.shape[1], hidden_layer_sizes=param)
        else:
            continue  # Add other hyperparameters as needed

        model.fit(X_train, y_train, epochs=epochs, verbose=0)
        train_score = model.evaluate(X_train, y_train, verbose=0)[1]
        test_score = model.evaluate(X_test, y_test, verbose=0)[1]

        train_scores.append(train_score)
        test_scores.append(test_score)

    plt.plot(param_range, train_scores, label='Training accuracy')
    plt.plot(param_range, test_scores, label='Validation accuracy')
    plt.xlabel(param_name)
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title(f'Validation Curve for {param_name}')
    plt.show()

# Define the range of values for the number of units in the hidden layer
param_name = 'hidden_layer_sizes'
param_range = [(10,), (50,), (100,), (150,), (200,)]

# Plot the validation curve
plot_validation_curve(param_name, param_range, create_model, X_train, y_train, X_test, y_test, epochs=50)


def plot_validation_curve_hidden_layers(hidden_layer_sizes, create_model_fn, X_train, y_train, X_test, y_test, epochs=100):
    train_scores = []
    test_scores = []

    for sizes in hidden_layer_sizes:
        model = create_model_fn(input_dim=X_train.shape[1], hidden_layer_sizes=sizes)

        model.fit(X_train, y_train, epochs=epochs, verbose=0)
        train_score = model.evaluate(X_train, y_train, verbose=0)[1]
        test_score = model.evaluate(X_test, y_test, verbose=0)[1]

        train_scores.append(train_score)
        test_scores.append(test_score)

    plt.plot([len(sizes) for sizes in hidden_layer_sizes], train_scores, label='Training accuracy')
    plt.plot([len(sizes) for sizes in hidden_layer_sizes], test_scores, label='Validation accuracy')
    plt.xlabel('Number of Hidden Layers')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Validation Curve for Number of Hidden Layers')
    plt.show()

# Define the range of values for the number of hidden layers
hidden_layer_sizes = [
    (50,),  # 1 layer
    (50, 50),  # 2 layers
    (50, 50, 50),  # 3 layers
    (50, 50, 50, 50),  # 4 layers
    (50, 50, 50, 50, 50)  # 5 layers
]

# Plot the validation curve
plot_validation_curve_hidden_layers(hidden_layer_sizes, create_model, X_train, y_train, X_test, y_test, epochs=50)

!pip install keras==2.12.0
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.wrappers.scikit_learn import KerasRegressor

# Function to create model, required for KerasClassifier
def create_model(hidden_layer_sizes=(50,), activation='relu'):
    model = Sequential()
    model.add(Dense(hidden_layer_sizes[0], input_dim=input_dim, activation=activation))
    for units in hidden_layer_sizes[1:]:
        model.add(Dense(units, activation=activation))
    model.add(Dense(1, activation='linear'))
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])
    return model

# Wrap the Keras model with KerasRegressor for use in scikit-learn
model = KerasRegressor(build_fn=create_model, verbose=0)

# Define the hyperparameter grid
param_grid = {
    'hidden_layer_sizes': [(50,), (50, 50), (50, 50, 50)],
    'epochs': [50, 100],
    'batch_size': [10, 20]
}

# Create the GridSearchCV object
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)

# Fit the grid search
grid_result = grid.fit(X_train, y_train)

# Summarize the results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# Evaluate the best model on the test set
best_model = grid_result.best_estimator_
test_mae = best_model.score(X_test, y_test)
print(f'Test MAE of the best model: {test_mae}')

!pip install scikeras

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from scikeras.wrappers import KerasRegressor
from sklearn.model_selection import GridSearchCV

# Load Wine Quality Dataset
wine_data_path = "/content/Wine-quality/winequality-white.csv"
wine_data = pd.read_csv(wine_data_path, delimiter=';')

# Preprocess the data
X = wine_data.drop('quality', axis=1)
y = wine_data['quality']

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Define the final model with the optimal hyperparameters
def create_final_model(input_dim):
    model = Sequential()
    model.add(Dense(50, input_dim=input_dim, activation='relu'))
    model.add(Dense(50, activation='relu'))  # Add another hidden layer
    model.add(Dense(1, activation='linear'))  # Use 'linear' activation for regression
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])
    return model

# Train the model with different training set sizes and plot learning curves
def plot_learning_curves_with_varying_data_size(X_train, y_train, X_test, y_test, model_fn, batch_size=20, epochs=100):
    # Define training set sizes
    train_sizes = np.linspace(0.1, 0.9, 9)  # Adjust range to avoid endpoints
    train_errors = []
    validation_errors = []

    for size in train_sizes:
        # Split the training data
        X_partial_train, _, y_partial_train, _ = train_test_split(X_train, y_train, train_size=size, random_state=42)

        # Create and train the model
        model = model_fn(input_dim=X_partial_train.shape[1])
        model.fit(X_partial_train, y_partial_train, epochs=epochs, batch_size=batch_size, verbose=0)

        # Evaluate the model on the training and validation sets
        y_train_predict = model.predict(X_partial_train)
        y_test_predict = model.predict(X_test)
        train_errors.append(mean_absolute_error(y_partial_train, y_train_predict))
        validation_errors.append(mean_absolute_error(y_test, y_test_predict))

    # Plot learning curves
    plt.plot(train_sizes, train_errors, label='Training error')
    plt.plot(train_sizes, validation_errors, label='Validation error')
    plt.xlabel('Training set size')
    plt.ylabel('Mean Absolute Error')
    plt.legend()
    plt.title('Learning Curves with Varying Training Set Size')
    plt.show()

# Example usage
plot_learning_curves_with_varying_data_size(X_train, y_train, X_test, y_test, create_final_model)

# Define a suboptimal model with fewer hidden layers
def create_suboptimal_model(input_dim):
    model = Sequential()
    model.add(Dense(50, input_dim=input_dim, activation='relu'))
    model.add(Dense(1, activation='linear'))  # Use 'linear' activation for regression
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])
    return model

# Train the suboptimal model and plot the learning curve
def plot_suboptimal_learning_curve(X_train, y_train, X_test, y_test, model_fn, batch_size=20, epochs=100):
    # Create and train the model
    model = model_fn(input_dim=X_train.shape[1])
    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=0)

    # Plot learning curves
    plt.plot(history.history['loss'], label='Training loss')
    plt.plot(history.history['val_loss'], label='Validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Learning Curve for Suboptimal Model')
    plt.show()

# Example usage
plot_suboptimal_learning_curve(X_train, y_train, X_test, y_test, create_suboptimal_model)


import time

# Measure training time for the optimal model
start_time = time.time()
optimal_model = create_final_model(input_dim=X_train.shape[1])
optimal_model.fit(X_train, y_train, epochs=100, batch_size=20, verbose=0)
end_time = time.time()
optimal_training_time = end_time - start_time
print(f'Optimal model training time: {optimal_training_time:.2f} seconds')

# Measure training time for the suboptimal model
start_time = time.time()
suboptimal_model = create_suboptimal_model(input_dim=X_train.shape[1])
suboptimal_model.fit(X_train, y_train, epochs=100, batch_size=20, verbose=0)
end_time = time.time()
suboptimal_training_time = end_time - start_time
print(f'Suboptimal model training time: {suboptimal_training_time:.2f} seconds')

import pandas as pd

# Define column names based on the dataset documentation
column_names = ['Class', 'Age', 'Menopause', 'Tumor Size', 'Inv Nodes', 'Node Caps',
                'Deg Malig', 'Breast', 'Breast Quad', 'Irradiat']

# Load Breast Cancer Dataset
breast_cancer_data_path = "/content/Breast/breast-cancer.data"
breast_cancer_data = pd.read_csv(breast_cancer_data_path, header=None, names=column_names)

# # Display the first few rows of the dataset
# print(breast_cancer_data.head())

# # Check for missing values
# print(breast_cancer_data.isnull().sum())

# # Summary statistics
# print(breast_cancer_data.describe())

# Handle missing values (if any)
# For example, we can fill missing values with the mode of the column (more appropriate for categorical data)
for column in breast_cancer_data.columns:
    if breast_cancer_data[column].isnull().any():
        breast_cancer_data[column].fillna(breast_cancer_data[column].mode()[0], inplace=True)

# # Verify that there are no more missing values
# print(breast_cancer_data.isnull().sum())

# # Optionally, display the first few rows to verify the data
# print(breast_cancer_data.head())


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Define column names based on the dataset documentation
column_names = ['Class', 'Age', 'Menopause', 'Tumor Size', 'Inv Nodes', 'Node Caps',
                'Deg Malig', 'Breast', 'Breast Quad', 'Irradiat']

# Load Breast Cancer Dataset
breast_cancer_data_path = "/content/Breast/breast-cancer.data"
breast_cancer_data = pd.read_csv(breast_cancer_data_path, header=None, names=column_names)

# Display the first few rows of the dataset
print(breast_cancer_data.head())

# Encode the target variable
y = breast_cancer_data['Class'].map({'no-recurrence-events': 0, 'recurrence-events': 1})

# Separate features and target
X = breast_cancer_data.drop('Class', axis=1)

# List of categorical and numerical columns
categorical_cols = ['Age', 'Menopause', 'Tumor Size', 'Inv Nodes', 'Node Caps', 'Breast', 'Breast Quad', 'Irradiat']
numerical_cols = ['Deg Malig']

# Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# Preprocessing for numerical data
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Combine preprocessors
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Apply the transformations
X_processed = preprocessor.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

# Verify the shapes of the training and testing sets
# print(f'X_train shape: {X_train.shape}')
# print(f'X_test shape: {X_test.shape}')
# print(f'y_train shape: {y_train.shape}')
# print(f'y_test shape: {y_test.shape}')

# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense

# Define the final model with the optimal hyperparameters
def create_final_model(input_dim):
    model = Sequential()
    model.add(Dense(50, input_dim=input_dim, activation='relu'))
    model.add(Dense(50, activation='relu'))  # Add another hidden layer
    model.add(Dense(1, activation='sigmoid'))  # Use 'sigmoid' activation for binary classification
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

import numpy as np
import matplotlib.pyplot as plt

import numpy as np
import matplotlib.pyplot as plt

# Train the model with different training set sizes and plot learning curves
def plot_learning_curves_with_varying_data_size(X_train, y_train, X_test, y_test, model_fn, batch_size=20, epochs=100):
    # Define training set sizes
    train_sizes = np.linspace(0.1, 0.9, 9)  # Adjust range to avoid endpoints
    train_errors = []
    validation_errors = []

    for size in train_sizes:
        # Split the training data
        X_partial_train, _, y_partial_train, _ = train_test_split(X_train, y_train, train_size=size, random_state=42)

        # Create and train the model
        model = model_fn(input_dim=X_partial_train.shape[1])
        model.fit(X_partial_train, y_partial_train, epochs=epochs, batch_size=batch_size, verbose=0)

        # Evaluate the model on the training and validation sets
        train_loss, train_acc = model.evaluate(X_partial_train, y_partial_train, verbose=0)
        val_loss, val_acc = model.evaluate(X_test, y_test, verbose=0)
        train_errors.append(train_acc)
        validation_errors.append(val_acc)

    # Plot learning curves
    plt.plot(train_sizes, train_errors, label='Training accuracy')
    plt.plot(train_sizes, validation_errors, label='Validation accuracy')
    plt.xlabel('Training set size')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Learning Curves with Varying Training Set Size')
    plt.show()

# Example usage
plot_learning_curves_with_varying_data_size(X_train, y_train, X_test, y_test, create_final_model)

import time

# Measure training time for the optimal model
start_time = time.time()
optimal_model = create_final_model(input_dim=X_train.shape[1])
optimal_model.fit(X_train, y_train, epochs=100, batch_size=20, verbose=0)
end_time = time.time()
optimal_training_time = end_time - start_time
print(f'Optimal model training time: {optimal_training_time:.2f} seconds')

# Define a suboptimal model with fewer hidden layers
def create_suboptimal_model(input_dim):
    model = Sequential()
    model.add(Dense(50, input_dim=input_dim, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))  # Use 'sigmoid' activation for binary classification
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Measure training time for the suboptimal model
start_time = time.time()
suboptimal_model = create_suboptimal_model(input_dim=X_train.shape[1])
suboptimal_model.fit(X_train, y_train, epochs=100, batch_size=20, verbose=0)
end_time = time.time()
suboptimal_training_time = end_time - start_time
print(f'Suboptimal model training time: {suboptimal_training_time:.2f} seconds')

import matplotlib.pyplot as plt

# Function to plot learning curve
def plot_learning_curve(model, X_train, y_train, X_test, y_test, epochs=100, batch_size=20):
    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)

    # Plot training & validation loss values
    plt.plot(history.history['loss'], label='Training loss')
    plt.plot(history.history['val_loss'], label='Validation loss')
    plt.title('Learning Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Create and train the model
optimal_model = create_final_model(input_dim=X_train.shape[1])
plot_learning_curve(optimal_model, X_train, y_train, X_test, y_test)


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Define the model creation function
def create_model(input_dim, hidden_layer_sizes=(50,)):
    model = Sequential()
    model.add(Dense(hidden_layer_sizes[0], input_dim=input_dim, activation='relu'))
    for units in hidden_layer_sizes[1:]:
        model.add(Dense(units, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model


import matplotlib.pyplot as plt

# Function to plot the validation curve for hidden layer sizes
def plot_validation_curve_hidden_layer_size(hidden_layer_sizes, create_model_fn, X_train, y_train, X_test, y_test, epochs=100):
    train_scores = []
    test_scores = []

    for size in hidden_layer_sizes:
        model = create_model_fn(input_dim=X_train.shape[1], hidden_layer_sizes=(size,))

        model.fit(X_train, y_train, epochs=epochs, verbose=0)
        train_score = model.evaluate(X_train, y_train, verbose=0)[1]
        test_score = model.evaluate(X_test, y_test, verbose=0)[1]

        train_scores.append(train_score)
        test_scores.append(test_score)

    plt.plot(hidden_layer_sizes, train_scores, label='Training accuracy')
    plt.plot(hidden_layer_sizes, test_scores, label='Validation accuracy')
    plt.xlabel('Hidden Layer Size')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Validation Curve for Hidden Layer Size')
    plt.show()

# Define the range of values for hidden layer sizes
hidden_layer_sizes_range = [25, 50, 75, 100, 125]

# Plot the validation curve
plot_validation_curve_hidden_layer_size(hidden_layer_sizes_range, create_model, X_train, y_train, X_test, y_test, epochs=50)


# Function to plot the validation curve for number of hidden layers
def plot_validation_curve_hidden_layers(hidden_layer_sizes, create_model_fn, X_train, y_train, X_test, y_test, epochs=100):
    train_scores = []
    test_scores = []

    for sizes in hidden_layer_sizes:
        model = create_model_fn(input_dim=X_train.shape[1], hidden_layer_sizes=sizes)

        model.fit(X_train, y_train, epochs=epochs, verbose=0)
        train_score = model.evaluate(X_train, y_train, verbose=0)[1]
        test_score = model.evaluate(X_test, y_test, verbose=0)[1]

        train_scores.append(train_score)
        test_scores.append(test_score)

    plt.plot([len(sizes) for sizes in hidden_layer_sizes], train_scores, label='Training accuracy')
    plt.plot([len(sizes) for sizes in hidden_layer_sizes], test_scores, label='Validation accuracy')
    plt.xlabel('Number of Hidden Layers')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Validation Curve for Number of Hidden Layers')
    plt.show()

# Define the range of values for the number of hidden layers
hidden_layer_sizes = [
    (50,),  # 1 layer
    (50, 50),  # 2 layers
    (50, 50, 50),  # 3 layers
    (50, 50, 50, 50),  # 4 layers
    (50, 50, 50, 50, 50)  # 5 layers
]

# Plot the validation curve
plot_validation_curve_hidden_layers(hidden_layer_sizes, create_model, X_train, y_train, X_test, y_test, epochs=50)

!pip install scikeras[tensorflow]

from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV

# Define the model creation function with additional parameters
def create_model(hidden_layer_sizes=(50,), input_dim=41):
    model = Sequential()
    for units in hidden_layer_sizes:
        model.add(Dense(units, input_dim=input_dim, activation='relu'))
        input_dim = units  # Update input_dim for the next layer
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Wrap the Keras model with KerasClassifier
model = KerasClassifier(build_fn=create_model, verbose=0, input_dim=X_train.shape[1])

# Define the hyperparameter grid
param_grid = {
    'hidden_layer_sizes': [(50,), (50, 50), (50, 50, 50)],
    'epochs': [50, 100],
    'batch_size': [10, 20]
}

# Create the GridSearchCV object
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)

# Fit the grid search
grid_result = grid.fit(X_train, y_train)

# Summarize the results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# Evaluate the best model on the test set
best_model = grid_result.best_estimator_
test_accuracy = best_model.score(X_test, y_test)
print(f'Test Accuracy of the best model: {test_accuracy}')