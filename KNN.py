# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16B-2I7N6xOoMTVMkV3wIAK9G-XizqK7J
"""

import pandas as pd
from sklearn.model_selection import train_test_split, validation_curve
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np

# Load Wine Quality Dataset
wine_data_path = "/content/Wine-quality/winequality-white.csv"  # Adjust path as needed
wine_data = pd.read_csv(wine_data_path, delimiter=';')

# Display the first few rows of the dataset
print(wine_data.head())

# Separate features and target
X = wine_data.drop('quality', axis=1)
y = wine_data['quality']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the parameter range for 'weights'
param_range = ['uniform', 'distance']

# Calculate training and test scores using validation_curve
train_scores, test_scores = validation_curve(
    KNeighborsClassifier(n_neighbors=5), X_train, y_train,
    param_name="weights", param_range=param_range,
    cv=5, scoring="accuracy", n_jobs=-1
)

# Calculate mean and standard deviation for training set scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)

# Calculate mean and standard deviation for test set scores
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot validation curve
plt.figure()
plt.plot(param_range, train_mean, label="Training score", color="red")
plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color="red", alpha=0.2)
plt.plot(param_range, test_mean, label="Cross-validation score", color="green")
plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color="green", alpha=0.2)
plt.title("Validation Curve with k-NN (weights)")
plt.xlabel("Weights")
plt.ylabel("Accuracy")
plt.legend(loc="best")
plt.grid()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, validation_curve
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import numpy as np

# Define column names based on the dataset documentation
column_names = ['Class', 'Age', 'Menopause', 'Tumor Size', 'Inv Nodes', 'Node Caps',
                'Deg Malig', 'Breast', 'Breast Quad', 'Irradiat']

# Load Breast Cancer Dataset
breast_cancer_data_path = "/content/breast/breast-cancer.data"  # Adjust path as needed
breast_cancer_data = pd.read_csv(breast_cancer_data_path, header=None, names=column_names)

# Display the first few rows of the dataset
print(breast_cancer_data.head())

# Encode the target variable
y = breast_cancer_data['Class'].map({'no-recurrence-events': 0, 'recurrence-events': 1})

# Separate features and target
X = breast_cancer_data.drop('Class', axis=1)

# List of categorical and numerical columns
categorical_cols = ['Age', 'Menopause', 'Tumor Size', 'Inv Nodes', 'Node Caps', 'Breast', 'Breast Quad', 'Irradiat']
numerical_cols = ['Deg Malig']

# Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Preprocessing for numerical data
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Combine preprocessors
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Apply the transformations
X_processed = preprocessor.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

# Define the parameter range for 'weights'
param_range = ['uniform', 'distance']

# Calculate training and test scores using validation_curve
train_scores, test_scores = validation_curve(
    KNeighborsClassifier(n_neighbors=5), X_train, y_train,
    param_name="weights", param_range=param_range,
    cv=5, scoring="accuracy", n_jobs=-1
)

# Calculate mean and standard deviation for training set scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)

# Calculate mean and standard deviation for test set scores
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot validation curve
plt.figure()
plt.plot(param_range, train_mean, label="Training score", color="red")
plt.plot(param_range, test_mean, label="Cross-validation score", color="green")
plt.title("Validation Curve with k-NN (weights)")
plt.xlabel("Weights")
plt.ylabel("Accuracy")
plt.legend(loc="best")
plt.grid()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
import numpy as np

# Load Wine Quality Dataset
wine_data_path = "/content/Wine-quality/winequality-white.csv"  # Adjust path as needed
wine_data = pd.read_csv(wine_data_path, delimiter=';')

# Display the first few rows of the dataset
print(wine_data.head())

# Separate features and target
X = wine_data.drop('quality', axis=1)
y = wine_data['quality']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the parameter grid
param_grid = {
    'weights': ['uniform', 'distance']
}

# Create the k-NN model
knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors as needed

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit the grid search
grid_search.fit(X_train, y_train)

# Get the best parameters and the best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best parameters found: {best_params}")
print(f"Best cross-validation accuracy: {best_score}")

# Evaluate the best model on the test set
best_model = grid_search.best_estimator_
test_accuracy = best_model.score(X_test, y_test)
print(f"Test accuracy of the best model: {test_accuracy}")

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Define column names based on the dataset documentation
column_names = ['Class', 'Age', 'Menopause', 'Tumor Size', 'Inv Nodes', 'Node Caps',
                'Deg Malig', 'Breast', 'Breast Quad', 'Irradiat']

# Load Breast Cancer Dataset
breast_cancer_data_path = "/content/breast/breast-cancer.data"  # Adjust path as needed
breast_cancer_data = pd.read_csv(breast_cancer_data_path, header=None, names=column_names)

# Display the first few rows of the dataset
print(breast_cancer_data.head())

# Encode the target variable
y = breast_cancer_data['Class'].map({'no-recurrence-events': 0, 'recurrence-events': 1})

# Separate features and target
X = breast_cancer_data.drop('Class', axis=1)

# List of categorical and numerical columns
categorical_cols = ['Age', 'Menopause', 'Tumor Size', 'Inv Nodes', 'Node Caps', 'Breast', 'Breast Quad', 'Irradiat']
numerical_cols = ['Deg Malig']

# Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Preprocessing for numerical data
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Combine preprocessors
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Apply the transformations
X_processed = preprocessor.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

# Define the parameter grid
param_grid = {
    'weights': ['uniform', 'distance']
}

# Create the k-NN model
knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors as needed

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit the grid search
grid_search.fit(X_train, y_train)

# Get the best parameters and the best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best parameters found: {best_params}")
print(f"Best cross-validation accuracy: {best_score}")

# Evaluate the best model on the test set
best_model = grid_search.best_estimator_
test_accuracy = best_model.score(X_test, y_test)
print(f"Test accuracy of the best model: {test_accuracy}")

import pandas as pd
from sklearn.model_selection import train_test_split, validation_curve
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import numpy as np

# Define column names based on the dataset documentation
column_names = ['Class', 'Age', 'Menopause', 'Tumor Size', 'Inv Nodes', 'Node Caps',
                'Deg Malig', 'Breast', 'Breast Quad', 'Irradiat']

# Load Breast Cancer Dataset
breast_cancer_data_path = "/content/breast/breast-cancer.data"  # Adjust path as needed
breast_cancer_data = pd.read_csv(breast_cancer_data_path, header=None, names=column_names)

# Display the first few rows of the dataset
print(breast_cancer_data.head())

# Encode the target variable
y = breast_cancer_data['Class'].map({'no-recurrence-events': 0, 'recurrence-events': 1})

# Separate features and target
X = breast_cancer_data.drop('Class', axis=1)

# List of categorical and numerical columns
categorical_cols = ['Age', 'Menopause', 'Tumor Size', 'Inv Nodes', 'Node Caps', 'Breast', 'Breast Quad', 'Irradiat']
numerical_cols = ['Deg Malig']

# Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Preprocessing for numerical data
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Combine preprocessors
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Apply the transformations
X_processed = preprocessor.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

# Define the parameter range for 'n_neighbors'
param_range = np.arange(1, 31)

# Calculate training and test scores using validation_curve
train_scores, test_scores = validation_curve(
    KNeighborsClassifier(weights='distance'), X_train, y_train,
    param_name="n_neighbors", param_range=param_range,
    cv=5, scoring="accuracy", n_jobs=-1
)

# Calculate mean and standard deviation for training set scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)

# Calculate mean and standard deviation for test set scores
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot validation curve
plt.figure()
plt.plot(param_range, train_mean, label="Training score", color="red")
plt.plot(param_range, test_mean, label="Cross-validation score", color="green")
plt.title("Validation Curve with k-NN (k values)")
plt.xlabel("Number of Neighbors (k)")
plt.ylabel("Accuracy")
plt.legend(loc="best")
plt.grid()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, validation_curve
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np

# Load Wine Quality Dataset
wine_data_path = "/content/Wine-quality/winequality-white.csv"  # Adjust path as needed
wine_data = pd.read_csv(wine_data_path, delimiter=';')

# Display the first few rows of the dataset
print(wine_data.head())

# Separate features and target
X = wine_data.drop('quality', axis=1)
y = wine_data['quality']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the parameter range for 'n_neighbors'
param_range = np.arange(1, 31)

# Calculate training and test scores using validation_curve
train_scores, test_scores = validation_curve(
    KNeighborsClassifier(weights='distance'), X_train, y_train,
    param_name="n_neighbors", param_range=param_range,
    cv=5, scoring="accuracy", n_jobs=-1
)

# Calculate mean and standard deviation for training set scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)

# Calculate mean and standard deviation for test set scores
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot validation curve
plt.figure()
plt.plot(param_range, train_mean, label="Training score", color="red")
plt.plot(param_range, test_mean, label="Cross-validation score", color="green")
plt.title("Validation Curve with k-NN (k values) for Wine Dataset")
plt.xlabel("Number of Neighbors (k)")
plt.ylabel("Accuracy")
plt.legend(loc="best")
plt.grid()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
import numpy as np

# Load Wine Quality Dataset
wine_data_path = "/content/Wine-quality/winequality-white.csv"  # Adjust path as needed
wine_data = pd.read_csv(wine_data_path, delimiter=';')

# Display the first few rows of the dataset
print(wine_data.head())

# Separate features and target
X = wine_data.drop('quality', axis=1)
y = wine_data['quality']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the parameter grid
param_grid = {
    'n_neighbors': np.arange(1, 31),
    'weights': ['uniform', 'distance']
}

# Create the k-NN model
knn = KNeighborsClassifier()

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit the grid search
grid_search.fit(X_train, y_train)

# Get the best parameters and the best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best parameters found: {best_params}")
print(f"Best cross-validation accuracy: {best_score}")

# Evaluate the best model on the test set
best_model = grid_search.best_estimator_
test_accuracy = best_model.score(X_test, y_test)
print(f"Test accuracy of the best model: {test_accuracy}")

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import numpy as np

# Define column names based on the dataset documentation
column_names = ['Class', 'Age', 'Menopause', 'Tumor Size', 'Inv Nodes', 'Node Caps',
                'Deg Malig', 'Breast', 'Breast Quad', 'Irradiat']

# Load Breast Cancer Dataset
breast_cancer_data_path = "/content/breast/breast-cancer.data"  # Adjust path as needed
breast_cancer_data = pd.read_csv(breast_cancer_data_path, header=None, names=column_names)

# Display the first few rows of the dataset
print(breast_cancer_data.head())

# Encode the target variable
y = breast_cancer_data['Class'].map({'no-recurrence-events': 0, 'recurrence-events': 1})

# Separate features and target
X = breast_cancer_data.drop('Class', axis=1)

# List of categorical and numerical columns
categorical_cols = ['Age', 'Menopause', 'Tumor Size', 'Inv Nodes', 'Node Caps', 'Breast', 'Breast Quad', 'Irradiat']
numerical_cols = ['Deg Malig']

# Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Preprocessing for numerical data
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Combine preprocessors
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Apply the transformations
X_processed = preprocessor.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

# Define the parameter grid
param_grid = {
    'n_neighbors': np.arange(1, 31),
    'weights': ['uniform', 'distance']
}

# Create the k-NN model
knn = KNeighborsClassifier()

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit the grid search
grid_search.fit(X_train, y_train)

# Get the best parameters and the best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best parameters found: {best_params}")
print(f"Best cross-validation accuracy: {best_score}")

# Evaluate the best model on the test set
best_model = grid_search.best_estimator_
test_accuracy = best_model.score(X_test, y_test)
print(f"Test accuracy of the best model: {test_accuracy}")

import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

# Function to plot learning curve without shadows
def plot_learning_curve(estimator, X, y, title="Learning Curve"):
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10), scoring='accuracy')

    train_scores_mean = np.mean(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)

    plt.figure()
    plt.title(title)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    plt.grid()

    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")

    plt.legend(loc="best")
    return plt

# Plot learning curve for wine dataset
best_knn_wine = KNeighborsClassifier(n_neighbors=20, weights='distance')
plot_learning_curve(best_knn_wine, X_train, y_train, title="Learning Curve for Wine Dataset")
plt.show()

import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

# Function to plot learning curve without shadows
def plot_learning_curve(estimator, X, y, title="Learning Curve"):
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10), scoring='accuracy')

    train_scores_mean = np.mean(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)

    plt.figure()
    plt.title(title)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    plt.grid()

    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")

    plt.legend(loc="best")
    return plt

# Plot learning curve for breast cancer dataset
best_knn_breast = KNeighborsClassifier(n_neighbors=11, weights='uniform')
plot_learning_curve(best_knn_breast, X_train, y_train, title="Learning Curve for Breast Cancer Dataset")
plt.show()